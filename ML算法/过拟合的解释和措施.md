# `过拟合的解释和措施`

### 定义：
机器学习任务包括一个训练集，测试集。

这两个一般有些差异（如果我们可以获取完备的真实数据集，那么我们压根就不需要做拟合了，我们只要查询就好了）。

然后训练的时候，我们定义在训练集的误差叫训练误差，在测试集的叫泛化误差。

Ltrain 很小 Ltest 很大 **过拟合**

Ltrain 很大 Ltest 很大 **欠拟合**

### 产生原因

通过训练数据，拟合数据的真实分布。

样本有可解释的规律部分（也就是其真实分布）共同规律，和噪声信息（每个样本都会存在的特异信息）组成，不可估计。训练集的有些特征收噪声影响，跟训练集存在较大差异。

当模型把噪声也当成有用特征进行拟合的时候，测试集就会出现过拟合。

### 解决方法

1、扩充数据，有利于学习一般化的规律，也就是训练集和测试集共有的。

2、约束模型：尽量让其简单，降低复杂度有利于学习通用化的特征。

    · 尽量挑选简单的模型。
    
    · 训练策略当验证集acc不提高的时候就停止训练。
    
    · 正则项惩罚模型。
    
### 正则化手段

**L1 loss**

w 更新
w:=w+α∂C0∂w+βλnsgn(w)
其中，梯度下降算法中，α<0,β<0，而在梯度上升算法中则相反。
从上式可以看出，当w为正时，更新后w会变小；当w为负时，更新后w会变大；
因此L1正则项是为了使得那些原先处于零（即|w|≈0）附近的参数w往零移动，使得部分参数为零，从而降低模型的复杂度（模型的复杂度由参数决定）。
相当于做了一个特征选择。

**L2 loss:**
减小参数值。模型对于微小扰动的反馈差异大实际就是一个过拟合的表现。
L2正则化可以对大数值权重进行惩罚，这使得没有哪个特征能够单独对整个模型有过大的影响，
即每个维度对最终结果的影响都不是很大，使模型把大多数维度上的特征都利用起来，而不是只依赖其中少数几个特征。

**dropout**

测试过程就是每个神经元的权重都乘以一个p，这样在“总体上”使得测试数据和训练数据是大致一样的。
比如一个神经元的输出是x，那么在训练的时候它有p的概率参与训练，
(1-p)的概率丢弃，那么它输出的期望是px+(1-p)0=px。因此测试的时候把这个神经元的权重乘以p可以得到同样的期望。
让更多节点共同作用。

**BN**
对于Mini-Batch SGD来说，一次训练过程里面包含m个训练实例，
其具体BN操作就是对于隐层内每个神经元的激活值，在激活层前。做一个归一化，减去均值，除以方差。

主要用途，**加速训练**，把输入值移到激活函数非饱和区。
当然这样会减小非线性（神经网络的拟合能力的主要体现），所以他还会加上一个偏移量ax'+b

对过拟合的帮助，做了一个batch归一化，对异常值没那么敏感。
  
