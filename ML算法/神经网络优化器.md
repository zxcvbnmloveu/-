# `神经网络优化器`

神经网络的目的是寻找合适的参数，使得损失函数的值尽可能小。解决这个问题的过程称为**最优化**。解决这个问题使用的算法叫做**优化器**。

#### SGD SGD做的是让参数往梯度方向移动学习率的步长。

![image](https://math.jianshu.com/math?formula=W%20%5Cleftarrow%20W%20-%20%5Ceta%20%5Cfrac%7B%5Cpartial%20L%7D%7B%5Cpartial%20W%7D)

```python3
class SGD:
    def __init__(self, lr=0.01):
        self.lr = lr
        
    def update(self, params, grades):
        for key in params.keys():
            params[key] -= self.lr * grades[key]
```

#### 优缺点
简单，容易实现。

缺点：低效

原因是，

1、延平缓方向移动缓慢，陡峭方向抖动剧烈（只跟当前batch梯度相关）。

2、会陷入局部最优值

#### Momentum SGD不稳定的原因是参数更新只依赖当前batch，加入惯性

![image](https://math.jianshu.com/math?formula=V_t%20%3D%20%5Cgamma%20V_%7Bt-1%7D%20%2B%20%5Ceta%20%5Cbigtriangledown_%5Ctheta%20J(%5Ctheta))

![image](https://math.jianshu.com/math?formula=%5Ctheta%20%3D%20%5Ctheta%20-%20V_t)

更新方向跟之前batch相关。增加了稳定性，收敛速度，有一定拜托局部最优的能力。

* 对于梯度改变方向的维度减少更新
* 对于梯度相同方向的维度增加更新

#### Nesterov（牛顿法）

NAG全称Nesterov Accelerated Gradient，是在SGD、SGD-M的基础上的进一步改进，改进点在于步骤1。我们知道在时刻t的主要下降方向是由累积动量决定的，自己的梯度方向说了也不算，那与其看当前梯度方向，不如先看看如果跟着累积动量走了一步，那个时候再怎么走。

![image](https://pic1.zhimg.com/80/v2-fd7d1b2ee3aa024a2504ee990ca10c5c_720w.png)



#### AdaGrad（Adaptive Gradient）

AdaGrad 基本思想是对每个变量用不同的学习率，这个学习率一开始比较大，用于快速梯度下降。随着优化过程的进行。对于已经下降很多的变量，减缓
学习率，对于还没怎么下降的变量，则保持一个较大的学习率。

<img src = "https://image.jiqizhixin.com/uploads/editor/8bd411db-4f1b-4d1d-94d1-ffc9dc067cb0/1527787914352.png" width = 40%/>


#### RMSProp（Adaptive Gradient）

由于AdaGrad单调递减的学习率变化过于激进，我们考虑一个改变二阶动量计算方法的策略：不累加全部历史梯度，而只关注过去一段时间窗口的下降梯度，
很自然的想到之前动量使用的指数加权平均，它所计算的就是过去一段时间的平均值，所以使用这一方法来计算二阶累积动量(滑动平均)。

#### Adam(AdaptiVe Moment Estimation)
Adam实际上就是将Momentum（梯度加上之前的值）和RMSprop（每个参数一个学习率）集合在一起。

<img src = "https://github.com/zxcvbnmloveu/MyNotes/blob/master/ML%E7%AE%97%E6%B3%95/pic/QQ%E6%88%AA%E5%9B%BE20201013162706.png" width = 40%/>




