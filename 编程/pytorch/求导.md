# `求导`

构建深度学习模型的基本流程就是：搭建计算图，求得预测值，进而得到损失，然后计算损失对模型参数的导数，再利用梯度下降法等方法来更新参数。

搭建计算图的过程，称为“正向传播”，这个是需要我们自己动手的，因为我们需要设计我们模型的结构。由损失函数求导的过程，称为“反向传播”。

如果要对tensor求导，需设置```.requires_grad_()  ```

Pytorch求解梯度的两个API

  * torch.autograd.grad(loss, [w1, w2, ...])
    提供哪些变量，就对哪些变量求导，当然这些变量可求导，返回[w1.grad, w2.grad, ...]
  * loss.backward()
    loss 可为一个向量，但是需传入一个对应loss 维度的Tensor，效果为加权函数
    返回所有的可求导变量的导数。

#### 简单例子
```python3
import torch

a = torch.tensor([1.,1.]).requires_grad_()
b = torch.tensor([2.,2.]).requires_grad_()
c = torch.tensor([3.,3.]).requires_grad_()

ans = torch.sum(a*b+c)
# 求导一般需要是一个数
print(ans)
print(a.grad,b.grad,c.grad)
ans.backward()
print(a.grad,b.grad,c.grad)

```

```
tensor(10., grad_fn=<SumBackward0>)
None None None
tensor([2., 2.]) tensor([1., 1.]) tensor([1., 1.])
```

参考 ![Pytorch-自动求导](https://www.jianshu.com/p/6418b3cf0d09)
