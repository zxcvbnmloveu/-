# `输出网络输入输出及参数`

众所周知，卷积网络输入计算十分麻烦，用summary可以输出每一层的输出，只要给一个标准输入1，2048，不用batchsize ，默认-1

```python3
from torchsummary import summary

class my_ae1(nn.Module):
    def __init__(self):
        super(my_ae1, self).__init__()
        self.encoder = nn.Sequential(
            nn.Conv1d(1, 64, 3, stride=2, padding=1),  # (b, 16, 10, 10)
            nn.BatchNorm1d(64),
            nn.ReLU(True),
            nn.MaxPool1d(2, stride=2),  # (b, 16, 5, 5)
            nn.Conv1d(64, 32, 3, stride=2, padding=2),  # (b, 8, 3, 3)
            nn.BatchNorm1d(32),
            nn.ReLU(True),
            nn.MaxPool1d(2, stride=1)  # (b, 8, 2, 2)
        )

        self.decoder = nn.Sequential(
            nn.ConvTranspose1d(32, 16, 3, stride=2),  # (b, 16, 5, 5)
            nn.BatchNorm1d(16),
            nn.ReLU(True),
            nn.ConvTranspose1d(16, 8, 3, stride=2, padding=1),  # (b, 8, 15, 15)
            nn.BatchNorm1d(8),
            nn.ReLU(True),
            nn.ConvTranspose1d(8, 1, 2, stride=2, padding=1),  # (b, 1, 28, 28)
            nn.Tanh()
        )
    def forward(self, x):
        # bz, d = x.size()[0], x.size()[1]
        # x = x.view(bz, 1, d)

        encode = self.encoder(x)
        decode = self.decoder(encode)

        # decode = decode.view(bz,d)
        return decode,encode

model1 = my_ae1().cuda()
summary(model1, (1,2048))
```

输出

```
        Layer (type)               Output Shape         Param #
================================================================
            Conv1d-1             [-1, 64, 1024]             256
       BatchNorm1d-2             [-1, 64, 1024]             128
              ReLU-3             [-1, 64, 1024]               0
         MaxPool1d-4              [-1, 64, 512]               0
            Conv1d-5              [-1, 32, 257]           6,176
       BatchNorm1d-6              [-1, 32, 257]              64
              ReLU-7              [-1, 32, 257]               0
         MaxPool1d-8              [-1, 32, 256]               0
   ConvTranspose1d-9              [-1, 16, 513]           1,552
      BatchNorm1d-10              [-1, 16, 513]              32
             ReLU-11              [-1, 16, 513]               0
  ConvTranspose1d-12              [-1, 8, 1025]             392
      BatchNorm1d-13              [-1, 8, 1025]              16
             ReLU-14              [-1, 8, 1025]               0
  ConvTranspose1d-15              [-1, 1, 2048]              17
             Tanh-16              [-1, 1, 2048]               0
================================================================
Total params: 8,633
Trainable params: 8,633
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.01
Forward/backward pass size (MB): 2.41
Params size (MB): 0.03
Estimated Total Size (MB): 2.45
----------------------------------------------------------------
```
